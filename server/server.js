import cors from 'cors';
import * as dotenv from 'dotenv';
import express from 'express';
import OpenAI from 'openai';

dotenv.config();

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const app = express();

app.use(cors());


// Handle CORS manually
// FIXED: change port from 5000 to 3001
// app.use((req, res, next) => {
//   res.setHeader('Access-Control-Allow-Origin', 'http://localhost:5173');
//   res.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS');
//   res.setHeader('Access-Control-Allow-Headers', 'Content-Type');

//   // Handle preflight
//   if (req.method === 'OPTIONS') {
//     return res.status(200).end();
//   }

//   next();
// });

app.use(express.json());

app.get('/', async(req, res) => {
  res.status(200).send({
    message: 'hello'
  })
})

app.post('/', async(req, res) => {
  try {
    const prompt = req.body.prompt;

    /**
     * OpenAI Chat Completion API åƒæ•¸èªªæ˜Ž
     *
     * @param {number} temperature - è¨­å¾—è¶Šé«˜ â†’ å›žç­”è¶Šæœ‰å‰µæ„ã€è¶Šå¤§è†½ã€è¶Šä¸ç¢ºå®šï¼›è¨­å¾—è¶Šä½Žï¼ˆä¾‹å¦‚ 0ï¼‰â†’ å›žç­”è¶Šç²¾æº–ã€å¯æŽ§ã€å¯é æ¸¬ã€‚
     * @param {number} max_tokens - é™åˆ¶æ¨¡åž‹æœ€å¤šèƒ½ç”¢ç”Ÿå¤šå°‘ tokenï¼ˆè©žç‰‡æ®µï¼‰ã€‚Token â‰  å­—æ•¸ã€‚è‹±æ–‡å¤§ç´„ 1 token â‰ˆ 0.75 å­—ã€‚ä¸­æ–‡å¤§ç´„ 1 token â‰ˆ 1 å­—ã€‚ðŸ‘‰ ç”¨ä¾†æŽ§åˆ¶å›žç­”é•·åº¦ï¼Œé¿å…å›žè¦†éŽé•·é€ æˆè²»ç”¨æˆ–è·‘ç‰ˆã€‚
     * @param {number} top_p - nucleus sampling æ ¸æŽ¡æ¨£ï¼Œå’Œ temperature ä¸€æ¨£æ˜¯æŽ§åˆ¶ã€Œå‰µæ„ vs ç²¾æº–ã€ã€‚top_p = 1 è¡¨ç¤ºä¸é™åˆ¶ï¼ŒæŽ¡æ¨£ç¯„åœæœ€å¤§ã€‚å¸¸è¦‹è¨­å®šï¼šå¸Œæœ›æ›´ä¿å®ˆ â†’ top_p = 0.5 ~ 0.9ã€‚âš ï¸ ä¸€èˆ¬å»ºè­°ï¼šä¸è¦åŒæ™‚èª¿é«˜ temperature å’Œ top_pï¼Œé¸ä¸€å€‹å³å¯ã€‚
     * @param {number} frequency_penalty - é »çŽ‡æ‡²ç½°ï¼ˆ-2 åˆ° 2ï¼‰æ•¸å€¼è¶Šé«˜ â†’ è¶Šä¸æœƒé‡è¤‡å‡ºç¾å·²ç¶“èªªéŽçš„å…§å®¹ã€‚
     * @param {number} presence_penalty - è©±é¡Œæ‡²ç½°ï¼ˆ-2 åˆ° 2ï¼‰æ•¸å€¼è¶Šé«˜ â†’ æ¨¡åž‹è¶Šä¸æœƒåœç•™åœ¨åŒä¸€ä¸»é¡Œï¼Œæœƒã€Œé¼“å‹µå®ƒè«‡æ–°è©±é¡Œã€ã€‚ä¾‹ï¼šè¨­ >0ï¼šé©åˆ brainstormã€æƒ³é»žå­ã€‚è¨­ 0ï¼šä¿æŒè‡ªç„¶ã€ä¸å¼·è¿«æ›è©±é¡Œã€‚
     */
    const response = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [{ role: "user", content: prompt }],
      temperature: 0,
      max_tokens: 3000,
      top_p: 1,
      frequency_penalty: 0.5,
      presence_penalty: 0,
    });

    res.status(200).send({
      bot: response.choices[0].message.content
    })

  } catch (error) {
    console.log(error)
    res.status(500).send({ error })

  }
})

const PORT = process.env.PORT || 3001;
app.listen(PORT, () => console.log(`Server is running on port ${PORT}`))